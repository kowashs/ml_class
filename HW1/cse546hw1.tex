\documentclass[11pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{braket}
\usepackage{cancel}
\usepackage{enumitem}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mathabx}
\usepackage{parskip}
\usepackage{tensor}
\usepackage{titlesec}
\usepackage{titling}
\usepackage{listings}


\setenumerate{leftmargin=*,label=\bf(\alph*)}


\titlelabel{(\thetitle)\quad}
\titleformat*{\section}{\large\bfseries}
\titleformat*{\subsection}{\normalsize\bfseries}
\setlength{\droptitle}{-5em}



\let\Re\relax
\DeclareMathOperator{\Re}{Re}
\let\Im\relax
\DeclareMathOperator{\Im}{Im}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\newcommand{\bhat}[1]{\hat{\bm{#1}}}


\renewcommand{\thesubsection}{\normalsize \alph{subsection}}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\del}{\vec{\nabla}}
\newcommand{\e}{\epsilon}
\newcommand{\tpd}[3]{\left( \frac{\partial #1}{\partial #2} \right)_{#3}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\spd}[2]{\frac{\partial^2 #1}{\partial {#2}^2}}
\def\dbar{{\mathchar'26\mkern-12mu d}}

\allowdisplaybreaks


\author{Sam Kowash}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\title{CSE 546 HW \#1}

\begin{document}
\maketitle


\section{MLE and bias--variance tradeoff}
\begin{enumerate}
	\item If we draw $x_1,\ldots,x_n \sim \text{uniform}(0,\theta)$ for some positive parameter $\theta$, then
	\begin{align*}
		\mathbb{P}(x_1,\ldots,x_n \mid \theta) &= \left\{\begin{array}{ll}
			\left(\frac{1}{\theta}\right)^n & \text{if } x_1,\ldots,x_n \in [0,\theta]\\
			0 & \text{else}\\
			\end{array}\right.
	\end{align*}
	%
	We can see that the likelihood increases as $\theta$ decreases as long as all points drawn still lie in $[0,\theta]$, so we conclude that the MLE is $\hat{\theta} = \max(x_1,\ldots,x_n)$.




	\item Let $(x_1,y_1),\ldots,(x_n,y_n) \in \mathbb{R}^d \times \mathbb{R}$ be drawn from some population. We define
	%
	\begin{align*}
	\hat{w} = \argmin_{w} \sum_{i=1}^n (y_i - w^T x_i)^2.
	\end{align*}
	%
	Let $(\tilde{x}_1,\tilde{y}_1),\ldots,(\tilde{x}_m,\tilde{y}_m)$ be test data drawn from the same population. We define the training and test losses
	%
	\begin{align*}
		R_\mathrm{tr} &= \frac{1}{n} \sum_{i=1}^n \left(y_i - \hat{w}^T x_i\right)^2,&
		R_\mathrm{te} &= \frac{1}{m} \sum_{i=1}^n \left(\tilde{y}_i - \hat{w}^T \tilde{x}_i\right)^2.
	\end{align*}
\end{enumerate}

\end{document}