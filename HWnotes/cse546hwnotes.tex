\documentclass[11pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{braket}
\usepackage{cancel}
\usepackage{enumitem}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{mathabx}
\usepackage{parskip}
\usepackage{tensor}
\usepackage{titlesec}
\usepackage{titling}


\setenumerate{leftmargin=*,label=\bf(\alph*)}


\titlelabel{(\thetitle)\quad}
\titleformat*{\section}{\large\bfseries}
\titleformat*{\subsection}{\normalsize\bfseries}
\setlength{\droptitle}{-5em}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\let\Re\relax
\DeclareMathOperator{\Re}{Re}
\let\Im\relax
\DeclareMathOperator{\Im}{Im}

\DeclareMathOperator{\sgn}{sgn}


\newcommand{\bhat}[1]{\hat{\bm{#1}}}


\renewcommand{\thesubsection}{\normalsize \alph{subsection}}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\del}{\vec{\nabla}}
\newcommand{\e}{\epsilon}
\newcommand{\tpd}[3]{\left( \frac{\partial #1}{\partial #2} \right)_{#3}}
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\spd}[2]{\frac{\partial^2 #1}{\partial {#2}^2}}
\def\dbar{{\mathchar'26\mkern-12mu d}}


\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

\allowdisplaybreaks


\author{Sam Kowash}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\title{CSE 546 HW \#notes}

\begin{document}
\maketitle


\section{LASSO}
Selects for sparse predictor $w$. We may want this either for efficiency or human-legibility. How do we do this? Well, greedy approach adds features one at a time based on improvement in test error, but that's pretty hacky. How do we know when to stop? How do we avoid just including a billion features?

Looking for sparse results is a type of regularization; we want to penalize feature overselection. This motivates the lasso objective:
%
\begin{align}
	\hat{w}_{lasso} &= \argmin_{w} \sum_{i=1}^n \left(y_i-x_i^T w\right)^2 + \lambda \|w\|_1.
\end{align}
%
This punishes big vectors $w$, which is what we want. Fact: for any $\lambda \geq 0$ for which $\hat{w}_r$ finds the minimum, there exists $\nu \geq 0$ such that
%
\begin{align}
	\hat{w}_r &= \argmin_{w} \sum_{i=1}^n \left(y_i - x_i^T w\right)^2 \, \text{subject to } r(w) \leq \nu.
\end{align}
%
That is, regularized regression problems can always be reframed as constrained optimization.

\begin{figure}[H]
	\centering
	\includegraphics[width=.6\textwidth]{figures/lasso_v_ridge.png}
	\caption{Lasso on left, ridge on right; lasso prefers solutions along coordinate axes (i.e. sparse)}
\end{figure}

If we incorporate an offset,
%
\begin{align}
	\hat{w},\hat{b} &= \argmin_{w,b} \sum_{i=1}^n \left(y_i - (x_i^T w + b)\right)^2 + \lambda \|w\|_1,
\end{align}
%
but joint optimization is a pain, so let's prefer to de-mean our data. This is still actually kind of tricky minimization; the 1-norm isn't differentiable at the origin and this complicates things. Do by coordinate descent, minimize one direction at a time. This is guaranteed to approach the optimum for lasso (which is nice), but how do we pick our order of coordinate descent? Options,
%
\begin{itemize}
	\item Random each time
	\item Round robin
	\item Try to pick ``important'' coordinates (biases us).
\end{itemize}

Let's see an example. Take $j \in \{1,\ldots,d\}$.
%
\begin{align}
	\sum_{i=1}^n \left(y_i - x_i^T w\right)^2 + \lambda \|w\|_1 &= \sum_{i=1}^n \left(y_i - \sum_{k=1}^d x_{i,k} w_k\right)^2 + \lambda \sum_{k=1}^d |w_k|\\
	&= \sum_{i=1}^n \left(\left(y_i- \sum_{k\neq j} x_{i,k} w_k \right) - x_{i,j} w_j \right)^2 + \lambda \sum_{k\neq j} |w_k| + \lambda |w_j|.
\end{align}
%
So set $\hat{w}_k = 0$ for $k \in \{1,\ldots,d\}$ and loop over points:
%
\begin{align}
	r_i^{(j)} &= \sum_{k\neq j} x_{i,j} \hat{w}_k\\
	\hat{w}_j &= \argmin_{w_j} \sum_{i=1}^n \left(r_i^{(j)} - x_{i,j} w_j \right)^2 + \lambda |w_j|.
\end{align}

Pulling out one 1-d problem at a time! Works b/c lasso objective is \emph{separable}. Except\ldots this isn't actually a lot better. Hard to optimize because of pointy bit. Need to extend some concept of derivative and convexity. Traditional definition for fn is that lines b/t points on $f(x)$ lie above $f(x)$ (epigraph is convex). We need a different one here:
%
\begin{align}
	f(y) &\geq f(x) + \nabla f(x)^T (y-x) \quad \forall x,y
\end{align}
%
This amounts to saying that there's a ``supporting hyperplane'' touching the epigraph at $x$ such that the epigraph lies entirely on one side of the plane. If the function is differentiable at $x$, this is going to be the tangent plane. If not, we may have \emph{many} options, and these are called \emph{subgradients} (denoted $\partial_{w_j}$ for subgradient set at $w_j$). We call differentiable functions extremized at $x$ where $\nabla f(x) = 0$, and similarly we will call other functions extremized when $f(x)$ admits 0 as a subgradient at $x$.

OK, so how do we actually take subgradients and set them to zero? Consider example,
%
\begin{align}
	\partial_{w_j} \left(\sum_{i=1}\left(r_i^{(j)} - x_{i,j} w_j\right)^2 + \lambda |w_j|\right) &= \left\{\begin{array}{lr}
		a_j w_j - c_j - \lambda & \text{if } w_j < 0\\
		\left[-c_j - \lambda, -c_j + \lambda\right] & \text{if } w_j = 0\\
		a_j w_j - c_j + \lambda & \text{if } w_j > 0\\
		\end{array}\right.,
\end{align}
%
where
%
\begin{align}
	a_j &= \left(\sum_{i=1}^n x_{i,j}^2\right), &
	c_j &= 2\left(\sum_{i=1}^n r_i^{(j)} x_{i,j}\right).
\end{align}
%
This tells us how to do our minimization (look for regime that contains zero as subgrad):
%
\begin{align}
	\hat{w}_j &= \left\{\begin{array}{lr}
		\frac{c_j + \lambda}{a_j} & \text{if } c_j < -\lambda\\
		0 & \text{if } |c_j|\leq \lambda\\
		\frac{c_j - \lambda}{a_j} & \text{if } c_j > \lambda\\
		\end{array}\right.
\end{align}
%
where, recall,
%
\begin{align}
	a_j &= \sum_{i=1}^n x_{i,j}^2 & c_j &= 2 \sum_{i=1}^n \left(y_i - \sum_{k\neq j} x_{i,k} w_k\right) x_{i,j}.
\end{align}
%
This central flattening behavior provides ``soft thresholding''; can make predictor entries identically zero depending on strength of regularization.






















\section{Classification problems}
Different from regression! Same principles though. Need a loss function; what is? Let's start with binary classification, want to learn $f: X \to Y$ where $X$ contains features, $Y \in \{0,1\}$ is target class. Natural loss is 0/1 function: $\bm{1} \{f(X) \neq Y \}$. Expected loss is then
%
\begin{align}
	\mathbb{E}_{XY} \left[\bm{1}\{f(X) \neq Y\}\right] &= \mathbb{E}_X \left[\mathbb{E}_{Y\mid X}\left[\bm{1}\{f(x) \neq Y\} \mid X=x\right]\right]\\
	&= \mathbb{E}_X \bigg\{ \bm{1}\{f(x)=1\}\mathbb{P}(Y=0 \mid X=x) + \bm{1}\{f(x)=0\}\mathbb{P}(Y=1 \mid X=x) \bigg\}.
\end{align}
%
Supposing we know $P(Y\mid X)$, the Bayes optimal classifier is 
%
\begin{align}
	f(x) &= \argmax_{y} \mathbb{P}(Y=y \mid X=x).
\end{align}

How do we actually estimate $\mathbb{P}(Y \mid X)$? Well, can't do linear, and we're out of tricks now. Need a new trick; some function that goes from $\mathbb{R}^d$ to $[0,1]$ (called link function). A nice option is the sigmoid/logistic curve:
%
\begin{align}
	\mathbb{P}(Y=0 \mid X,W) &= \frac{1}{1+\exp\left[w_0 + \sum_i w_i X_i\right]}.
\end{align}
%
Still a sort of linear model in terms of what we do to our data variables. Note that $w_0$ applies a horizontal shift, and the $w_i$ ``stretch'' the curve. Note a nice property for binary classification, which is that
%
\begin{align}
	\frac{\mathbb{P}(Y=1 \mid w,X)}{\mathbb{P}(Y=0 \mid w,X)} &= \exp\left[w_0 + w^T X\right].
\end{align}
%
Reasonable to make our rule to classify as 1 if ratio is greater than 1, classify as 0 if ratio is less than 1. Equivalent result,
%
\begin{align}
	\ln \frac{\mathbb{P}(Y=1 \mid w,X)}{\mathbb{P}(Y=0 \mid w,X)}  = w_0 + \sum_i w_i X_i \to \left\{
		\begin{array}{l}
			< 0 \implies \text{ classify as 0}\\
			> 0 \implies \text{ classify as 1}\\
		\end{array}\right..
\end{align}

Alternative formulation (conditional likelihood): say $y \in \{-1,1\}$ instead of $\{0,1\}$ so we can then write
%
\begin{align}
	\mathbb{P}(Y=y \mid x,w) &= \frac{1}{1+\exp\left(-y w^T x\right)}
\end{align}
%
and find the MLE:
%
\begin{align}
	\hat{w}_\mathrm{MLE} &= \argmax_w \prod_{i=1}^n \mathbb{P}(y_i \mid x_i,w)\\
	\hat{w}_\mathrm{MLE} &= \argmin_w \sum_{i=1}^n \underbrace{\log\left(1+ \exp\left(-y_i x_i^T w\right) \right)}_{\sigma(y_i x_i^T w)}
\end{align}
%
Call the objective $J(w)$ (logistic loss): what the hell does it look like? Is it convex? (Yes.) How do we minimize it? Note that for an argument $z$, $\sigma(z) \sim |z|$ as $z \to -\infty$ and $\sigma(z) \to 0$ as $z \to +\infty$. In between, does some nonsense. So $\sigma(z)$ is easy to understand, but what about $J(w)$? Generally, no closed form in terms of $w$, can't just take derivative and set to zero. It turns out, moreover, that if we happen to have a $w$ that correctly classifies every point (by dumb luck; linearly separable data set), then $y_i x_i^T w$ is strictly positive and so for $t > 0$, $y_i x_i^T (tw) > y_i x_i^T w$ and we will always prefer to infinitely extend our $w$ along our ``good'' direction to continue reducing loss. This breaks the classifier. Sigmoid classifiers become step functions. We \emph{need} to regularize this conditional log likelihood objective.













\section{Interlude: Gradient descent}
Recall that our general problem is to be handed i.i.d. data $\{(x_i,y_i)\}_{i=1}^n$ with $x_i \in \mathbb{R}^d$ and $y_i \in \mathbb{R}$, a model with some parameters $w$, and some loss function depending on $w$ to be calculated on our data, then optimize $w$ with respect to that loss. The computational meat is in optimization. We turn it into a problem that looks like $X^T X w = X^T Y$, but how do we actually do that? Lots of different ways! Many algorithms! Some depend on properties of matrices, sparseness, etc. Let's look at one in particular, which is gradient descent.

Say I have some function $f(x)$ where $x \in \mathbb{R}^d$. Taylor tells us that
%
\begin{align}
	f(x + \delta) &= f(x) + \nabla f(x) \cdot \delta + \cdots,
\end{align}
%
so at any point we can find the gradient and take a step in the opposite direction, which must decrease $f$. Ex:
%
\begin{align}
	f(w) &= \frac{1}{2} \|X w - y\|_2^2\\
	\nabla f(w) &= X^T(Xw -y) = X^T (Xw -y) = X^T X w - X^T y,
\end{align}
%
so if we are at some $w_t$ our next step should be
%
\begin{align}
	w_{t+1} &= w_t - \eta X^T(X w_t - y).
\end{align}
%
Examine recursion behavior, find (for optimum $w_\ast$) that
%
\begin{align}
	(w_{t+1} - w_\ast) &= \left(I - \eta X^T X \right)^{t+1} (w_0 - w_\ast),
\end{align}
%
so can bound rate of approach to optimum. (Shocker: need $\eta$ small.)


We can do one better by going one step further down the Taylor series,
%
\begin{align}
	f(y) &\approx f(x) + f'(x) (y-x) + \frac{1}{2} f''(x) (y-x)^2
\end{align}
%
make our next step the minimum
%
\begin{align}
	\tilde{y} &= \frac{f'(x)}{f''(x)}
\end{align}










\section{Perceptron}
Recall binary classification: we want to learn $f: X \to Y$ with loss $\ell(f(x),y) = \bm{1}\{f(x)\neq y\}$. Expected loss
%
\begin{align}
	\mathbb{E}_{XY} \left[\bm{1}\{f(X)\neq Y\}\right] &= \mathbb{E}_X\left[\mathbb{E}_{Y\mid X} \left[\bm{1}\{f(x) \neq Y\}\mid X=x\right]\right]\\
	\mathbb{E}_{Y\mid X} \left[\bm{1}\{f(x) \neq Y\}\mid X=x\right] &= 1 - P\left(Y=f(x) \mod X=x\right),
\end{align}
%
so obviously want Bayes classifier
%
\begin{align}
	f(x) &= \argmax_{y} \mathbb{P}\left(Y=y\mid X=x\right),
\end{align}
%
and often take a logistic model like
%
\begin{align}
	P(Y=y\mid x,w) &= \frac{1}{1+ \exp(-yw^T x)}.
\end{align}
%
Buuuuuut it's really hard to know if that model's good. Often probably isn't. Can we do this without a model? Certainly it's not hard to do it with our eyeballs. Can we give our computer eyeballs? Yup!

Say we're classifying to $y \in \{-1,1\}$ and take a linear model where we predict $\sgn(w^T x + b)$. Start with $w_0 = 0, b_0 = 0$, get a data point $x_k$, predict $y_k$, check, and if we were wrong, update
%
\begin{align}
	\begin{bmatrix}w_{k+1}\\b_{k+1}\\\end{bmatrix} &= \begin{bmatrix}w_k\\b_k\\\end{bmatrix} + y_k \begin{bmatrix}x_k\\1\\\end{bmatrix}.
\end{align}
%
Basically we rotate/shift our classification line until we get everything right. Guaranteed to converge if we have linearly separable data! If $\gamma$ is the width of the widest margin (hyperplanar slab separating classes) and feature vector norms are bounded by $\|x_k\| \leq R$, then there is a theorem (Block, Novikoff): if we have a sequence of examples $(x_t,y_t)$, then the number of mistakes made by the perceptron is bounded by $R^2 / \gamma^2$ for \emph{any} such sequence. Convergence is guaranteed and \emph{fast}.

This is pretty dank, but\ldots we can't really do much with it. Most things aren't linearly separable (or even separable at all!), and even a single point in the way of separability can cause infinite cycling and destroy convergence. So why do we care at all? Well, it's a totally different way of thinking about learning! Previously we wrote down a model and then developed an algorithm that optimizes it. Here we did exactly the opposite: wrote down an algorithm and analyzed it. Is there an equivalent modeling problem? What is the perceptron optimizing? Does it have a definable loss function? This all leads to support vector machines (SVMs).

















\section{Support vector machines}
The perceptron model admitted many possible classifiers for a given data set, and it liked them all just as well, but we are not so generous. Some seem to skirt very close to the edges of our classes, which feels dangerous, and we'd be more comfortable with a boundary as far from our known data as possible, considering that our data may suffer perturbations on future collections.

How can we pick the ``safest'' classifier? Consider some hyperplane boundary $x^T w + b = 0$ that we've trained, and the closest point $x_0$ to it. How close is it? Let
%
\begin{align}
	\tilde{x}_0 &= \argmin_{z \in \{x : x^T w + b = 0\}} \|z - x_0 \|_2^2
\end{align}
%
be the closest point on the hyperplane to $x_0$. Then
%
\begin{align}
	\|x_0 - \tilde{x}_0\|_2 &= \left\|\frac{w^T}{\|w\|_2} (x_0-\tilde{x}_0)\right\|,
\end{align}
%
but we know that $w^T \tilde{x}_0 = -b$, so
%
\begin{align}
	\|x_0 - \tilde{x}_0\|_2 &= \frac{|w^T x_0 + b|}{\|w\|_2}.
\end{align}
%
So if we now call $\gamma$ the distance from the classification boundary to a parallel hyperplane on either side, our problem is to maximize $\gamma$ over $w$ and $b$ subject to
%
\begin{align}
	\frac{y_i(x_i^T w + b)}{\|w\|_2} &\geq \gamma \qquad \forall i
\end{align}
%
where the sign $y_i$ makes sure we're paying attention to points on both sides of the boundary correctly. This problem ends up being equivalent, however, to the minimization of $\|w\|_2^2$ subject to
%
\begin{align}
	y_i\left(x_i^T w + b\right) & \geq 1 \qquad \forall i,
\end{align}
%
which gives us both our optimal hyperplane and a measure of our separability for free.




However, this still breaks for non-separable data! To make this useful, we need to permit some misclassification or a smaller margin! We implement this as minimizing $w$ subject to
%
\begin{align}
	y_i(x_i^T w + b) &\geq 1 -\xi_i\\
	\xi_i &\geq 0\\
	\sum_j \xi_j &< \nu.
\end{align}
%
We now have a knob to trade off between $\nu$ and $\|w\|_2$. The $\xi_i$ are called support vectors (samples on the margin).

Constrained optimization sucks, but as we found with lasso, we have some tricks. Specifically, for any $\nu \geq 0$, there is a $\lambda \geq 0$ such that we can just as well minimize
%
\begin{align}
	\sum_{i=1}^n \max\{0, 1 - y_i(b + x_i^T w)\} + \lambda \|w\|_2^2.
\end{align}

We're back in machine learning land, now! We have a loss function
%
\begin{align}
	\ell_i(w) &= \max\{0, 1 - y_i x_i^T w\}
\end{align}
%
which we call the \emph{hinge loss} and can optimize by any of the methods we've talked about before. Fact: this is exactly the loss being optimized by the perceptron!
%
\begin{align}
	\nabla_w \ell_i(w,b) &= \left\{
	\begin{array}{ll}
		-y_i x_i + \frac{2\lambda}{n} w & \text{if } 1-y_i(b+x^T w) > 0\\
		\frac{2\lambda}{n} w & \text{otherwise}\\
	\end{array}\right.	
\end{align}
%
SGD update for this objective is the perceptron with the right parameters! (The kink in hinge loss is what gives us the ``give up'' behavior in perceptron?) Note that Newton's method does not work on hinge loss, but it has subgradients so SGD still works.


Note that logistic regression assumed a data model $\mathbb{P}(Y=y \mid X=x, w)$ that's pretty rigid. It gives us confidence measurements, but how confident are we in our confidence? SVM doesn't force us to assume such a model, so can we extract confidence that we trust from that? Yes! Our SVM predictor gives us a bunch of predictions $y_i = \sgn(w^T x_i)$. Isotonic regression is the fitting of a function to that data that interpolates those predictions (possibly logistic).






\section{Bootstrap}
So cross-validation was pretty good technology, but it has downsides, for example throwing out a fifth of our data. The validation error was useful, but the only real statistical power was for constraining the mean via Hoeffding's inequality, which doesn't help with other statistical figures. Further, we get errors for the population, but not really for individual data points. Enter the bootstrap black magicks.

Say we have a dataset $\mathcal{D} = \{z_1,\ldots,z_n\}$ drawn i.i.d from some CDF $F_Z$. We want to compute a statistic $\hat{\theta} = t(\mathcal{D})$ (say the variance). We can calculate the empirical CDF
%
\begin{align}
	\hat{F}_{n,Z}(x) &= \frac{1}{n} \sum_{i=1}^n \bm{1}\left\{z_i \leq x\right\}
\end{align}
%
and we know that $\mathbb{E}\left[\hat{F}_{n,Z}(x)\right] = F_Z(x)$. Say I could get more data: $\{\mathcal{D}_1,\mathcal{D}_2,\ldots,\mathcal{D}_b\}$ from the same distribution. Then I can get a distribution of $\hat{\theta}_i = t(\mathcal{D}_i)$ and can compute stats on stats on stats!

But it's hard to get a bunch of data sets, often! Fortunately we have an estimate of $F_Z(x)$ lying around, which is to say $\hat{F}_{n,Z}(x)$; why not draw our new data sets from that? We know that it will converge toward the true CDF for large $n$. So let's generate $\mathcal{D}^{\ast b} = \{z_1^{\ast b}, \ldots, z_n^{\ast b}\}$ drawn from $\hat{F}_{n,Z}$ with replacement and get $\theta^{\ast b} = t(\mathcal{D}^{\ast b})$ then compute statistics on these $\theta^{\ast b}$, which should be good estimates of the true statistics.


What do we do with this? Whatever we want! We can fit a function to some data, then fit 1000 more with bootstrap and get a confidence interval for \emph{every single point}. This is crazy. Caveat: need lots of data. It has powerful asymptotic guarantees, but they don't really become good constraints until big $n$ and it's hard to tell how big is big enough. Further, this can involve a lot of computation if you want tight bounds, and there are statistics that it does not work well for.











\section{Decision theory; discriminative vs. generative learning}
Recall in binary classification, we talked about classifying according to
%
\begin{align}
	f(x) &= \argmax_y \mathbb{P}(Y=y \mid X=x),
\end{align}
%
but it's totally equivalent according to Bayes to look at
%
\begin{align}
	f(x) &= \argmax_y \mathbb{P}(X=x \mid Y=y) \mathbb{P}(Y=y),
\end{align}
%
where we think about the distribution of features given a certain outcome. Need to scale based on probability of outcome to predict!


Consider an example where
%
\begin{align}
	\mathbb{P}(X=x) &\equiv (1-\pi) P_0(x) + \pi P_1(x)
\end{align}
%
where $0\leq\pi \leq 1$ and $P_0(x) = \mathcal{N}(x; \mu_0, \sigma^2)$ and $P_1(x) \mathcal{N}(x;\mu_1,\sigma^2)$. The Bayes classifier is
%
\begin{align}
	f(x) = 1 \text{ if } \frac{P_1(x) \pi}{P_0(x) (1-\pi)} \geq 1.
\end{align}
%
This comes out to
%
\begin{align}
	f(x) = 1 \text{ if } x \geq \frac{\mu_1 +\mu_0}{2} - \frac{\sigma^2}{\mu_1 - \mu_0} \log\left(\frac{\pi}{1-\pi}\right).
\end{align}
%
Can see that as $\pi$ grows, decision boundary shifts left, and vice-versa.

This generalizes to more dimensions! If we have Gaussian feature distributions for each class with the same covariance, we will get a hyperplane classification boundary (linear discriminant analysis). If they have different covariances, quadratic boundary (quadratic discriminant analysis).

Say we measure $\{(x_i,y_i)\}_{i=1^n}$, then we get estimates
%
\begin{align}
	\hat{mu}_k &= \frac{1}{|\{i : y_i = k\}} \sum_{i:y_i=k} x_i\\
	\hat{Sigma}_k &= \frac{1}{|\{i:y_i = k\}|-1} \sum_{i:y_i = k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T.
\end{align}

So discriminative learning directly attempts to model $\mathbb{P}(Y=y \mid X=x)$ (e.g. SVM, logistic classification), while generative learning goes after $\mathbb{P}(X=x,Y=y) = \mathbb{P}(X=x\mid Y=y) \mathbb{P}(Y=y)$. Is this more work? Yes! But often domain knowledge allows us to do some educated preparation.













\section{Hypothesis testing}
This is\ldots obviously important.

Say that we want to detect anomalous transactions in a credit log. For each transaction we observe some features $X$, and the transaction is either genuine ($Y=0$) or fraudulent ($Y=1$). In the first case $H_0$, $X$ is drawn from a distribution $P_0$ for genuine transactions, and in $H_1$ it is drawn from $P_1$. We want to determine which population our feature vector is drawn from by some (possibly randomized) decision function $\delta(X)$.

In Bayesian hypothesis testing, we assume priors $\mathbb{P}(Y=1) = \pi$ and $\mathbb{P}(Y=0) = (1-\pi)$ and want to minimize $\mathbb{P}_{XY}(Y \neq \delta(X))$. (It turns out that there will always be deterministic dudes that live in this set.) Effectively this is weighting distributions for decision according to priors.


We can retune our priorities by instead looking for
%
\begin{align}
	\argmin_\delta \max\{\mathbb{P}(\delta(X) = 0\mid Y=1), \mathbb{P}(\delta(X) =1 \mid Y=0)\},
\end{align}
%
which is known as minimax hypothesis testing.



There is also Neyman--Pearson hypothesis testing where we choose
%
\begin{align}
	\argmax_\delta \mathbb{P}(\delta(X) = 1 \mid Y=1) \text{ subject to } \mathbb{P}(\delta(X)=1\mid Y=0) \leq \alpha.
\end{align}
%
This maximizes true positives subject to a bounded false alarm rate $\alpha$. The wacky thing is that there is always an optimal test $\delta^\ast$ following
%
\begin{align}
	\mathbb{P}(\delta^\ast(X) =1) &= \left\{\begin{array}{lr}
		1 & \text{ if } \frac{P_1(x)}{P_0(x)} > \eta\\
		\gamma & \text{ if } \frac{P_1(x)}{P_0(x)} = \eta\\
		0 & \text{ if } \frac{P_1(x)}{P_0(x)} < \eta
	\end{array}\right..
\end{align}
%
such that the false alarm rate is \emph{exactly} $\alpha$.

All hypothesis tests are effectively a trade-off between false alarm rate and detection power. Different algorithms will have different trade-off curves (ROC curves), and we can assess them on this basis.














\subsection{p-values}
We are often confronted with problems where we (hopefully) have a strong model for the null hypothesis distribution, but no real model for non-null hypotheses (e.g. real vs. fraudulent purchases, since fraud is often strategic/adaptive). Can we test here? Yuuuup.



One definition is that the p-value is the probability of finding data at least as extreme if the null hypothesis is true (that is, $X \sim P_0$). Alternatively, the p-value is a uniform r.v. derived from $X$ in the case that $X \sim P_0$. (And note: \emph{any} such uniform variable. There are many possible definitions!) This second rule will do us more favors. The first might lead us to suspect that a particular $H_1$ is true, and that's very bad.

Say that we have $P_0(x) = \mathcal{N}(x;\mu_0, \sigma^2)$ and observe $x_i \in \mathbb{R}$. The p-value is
%
\begin{align}
	p_i &= P_0(X \geq x_i)\\
	    &= \int_{x=x_i}^\infty \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu_0)^2}{2\sigma^2}} \d x\\
	    &= 1 - F\left(\frac{x_i-\mu_0}{\sigma}\right).
\end{align}
%
The correct way to use this information is to pick a threshold (e.g. $\alpha = 0.05$), measure data $x_i$, then calculate $p_i$ (uniform on $[0,1]$ under $H_0$). Then the test is that if $p_i \leq \alpha$, we reject $H_0$. (Or, more generally, we can define any weird rejection region we want, as long as it has measure no greater than $\alpha$.)


Note that the uniformity of $p$ under $H_0$ presents a danger: repeated measurements can easily land us in the rejection region by chance! Say we measure $x_i \sim \mathcal{N}(\mu,1)$ each day $i$ and take $H_0: \mu=0$. We can define a running average
%
\begin{align}
	Z_i &= \frac{1}{\sqrt{i}} \sum_{j=1}^i x_j
\end{align}
%
and under $H_0$ we have $Z_i \sim \mathcal{N}(0,1)$ so that we can define a $p$-value
%
\begin{align}
	p_i &= \frac{1}{2\pi} \int_{z=z_i}^\infty e^{-\frac{z^2}{2}} \d z.
\end{align}
%
The danger is that \emph{eventually} we will measure some $p_i \leq \alpha$. If we declare victory here, our stopping time was not independent of our measurements; it is much like tainting our learning process by touching the test data. Caution!










\subsection{Multiple testing}
Take a genetics example: 13,071 drosophila genes potentially affecting virus replication. Knock out one gene at a time, infect microwell array, count fluorescent signals distributed $x_i \sim \mathcal{N}(\mu_i,1)$ (not actually, but play along).

For each gene $i$, we have a null hypothesis $H_0(i) : \mu_i = 0$, i.e. it plays no role in virus inhibition. Procedure:
%
\begin{description}
	\item[Set:] $\alpha=0.05$
	\item[Observe:] $x_i \in \mathbb{R}$
	\item[p-value:] $p_i = P_0(X \geq x_i)$
	\item[Test:] If $p_i \leq \alpha$, reject $H_0(i)$
\end{description}

How many genes do we expect to meet the rejection criterion under $H_0$? If $I_0$ is the set of $i$ for which $H_0(i)$ is true,
%
\begin{align}
	\mathbb{E}\left[\sum_{i\in I_0} \bm{1}\{p_i \leq \alpha\}\right] &= \sum_{i \in \mathbb{I}_0} \mathbb{P}(p_i \leq \alpha) = |I_0| \alpha.
\end{align}
%
This is an\ldots alarming rate of false alarms. We can measure this badness by the family-wise error rate (FWER) which is the probability of rejecting any true null. If we adopt the Bonferroni rule (reject $i$ if $p_i \leq \alpha/n$), then
%
\begin{align}
	\mathrm{FWER} &= \mathbb{P}\left(\bigcup_{I_0} \{p_i \leq \frac{\alpha}{n}\}\right)\\
	&= \leq \sum_{i \in I_0} \mathbb{P}\left(p_i \leq \frac{\alpha}{n}\right)
	&= \sum_{i \in I_0} \frac{}{}
\end{align}




This might be a little harsh. Perhaps better to judge by false discovery rate (FDR); we can tolerate a few errors if we get enough true discoveries.
%
\begin{align}
	\mathrm{FDR} &\equiv \mathbb{E}\left[\frac{|I_0 \cap R|}{|R|}\right]
\end{align}
%
where $R$ is the rejection set. There is a procedure called the Benjamini--Hochberg procedure:
%
\begin{itemize}
	\item Sort p-values so that $p_{(1)} \leq p_{(2)} \leq \cdots \leq p_{(n)}$
	\item $i_\mathrm{max} = \max\left\{i : p_{(i)} \leq \frac{i}{n} \alpha\right\}$
	\item $R = \{i: i \leq i_\mathrm{max}\}$
\end{itemize}
%
where $R$ is our rejection set. A theorem tells us that $BH(\alpha)$ satisfies $\mathrm{FDR} \leq \alpha$.











\section{Bayesian methods}
Everything we've just been over is pretty classical, nothing really new after 1900. Let's move on a bit. Recall our MLE example about determining the weighting $\theta$ of a coin. Suppose we were told that based on past knowledge the coin is around 50--50, so we have some \emph{prior belief} $P(\theta)$. Bayes sez
%
\begin{align}
	P(\theta \mid \mathcal{D}) &= \frac{P(\mathcal{D}\mid \theta) P(\theta)}{P(\mathcal{D})}.
\end{align}
%
The likelihood function, we know, is the binomial
%
\begin{align}
	P(\mathcal{D} \mid \theta) &= \theta^{\alpha_H} (1- \theta)^{\alpha_T},
\end{align}
%
where $\alpha_H, \alpha_T$ are our drawn data. 













\end{document}